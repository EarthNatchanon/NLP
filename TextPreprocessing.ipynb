{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6233d4-5032-4d04-bcc6-6eec45ee74eb",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "## ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å\n",
    "> #### ‡∏ô‡∏≤‡∏¢‡∏ì‡∏±‡∏ê‡∏ä‡∏ô‡∏ô ‡∏®‡∏£‡∏µ‡∏î‡∏ß‡∏á‡∏©‡πå | 660710114\n",
    "> #### ‡∏ô‡∏≤‡∏¢‡∏ò‡∏ô‡∏¥‡∏ô‡∏ó‡πå ‡∏ï‡∏±‡πâ‡∏á‡∏Å‡∏≠‡∏ö‡∏•‡∏≤‡∏† | 660710714"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f6a36-084f-4948-b45b-9687a3c3806c",
   "metadata": {},
   "source": [
    "### dataset from kaggle\n",
    "> ##### Click here for more info -> [SMS Spam Collection Dataset](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cea0b6-65c0-4293-934c-8417c80e188b",
   "metadata": {},
   "source": [
    "### ‡∏™‡∏≥‡∏£‡∏ß‡∏à dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe254a13-eab3-45a6-8e1a-e63254ce0286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"spam.csv\", encoding='latin1')\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71bd662-374f-4302-92f7-bda711cb8fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1               0\n",
       "v2               0\n",
       "Unnamed: 2    5522\n",
       "Unnamed: 3    5560\n",
       "Unnamed: 4    5566\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e67e7b-6300-41e5-a0ff-f9bc6b793fc0",
   "metadata": {},
   "source": [
    "#### ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Unnamed 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd3dccd-35d5-4791-b529-9ca8f0f0c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')\n",
      "After drop columns Index(['v1', 'v2'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.columns)\n",
    "df = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "print(\"After drop columns\", df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c9a21-348c-4900-90db-c4a1a48315c2",
   "metadata": {},
   "source": [
    "#### ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 5572 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeba6c2e-eac6-4b89-a967-3cd4271433b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f966952e-ac7d-4a19-ade2-04f0eff54c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range (df.shape[0]): #df.shape[0] = 5572\n",
    "#     print(i, df.loc[i,'v2'])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29013f-cbd3-4351-abba-239c96e6896a",
   "metadata": {},
   "source": [
    "#### ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏î‡∏π‡∏à‡∏∞‡∏û‡∏ö‡∏ß‡πà‡∏≤ ‡πÄ‡∏à‡∏≠‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÅ‡∏õ‡∏•‡∏Å‡πÜ‡πÄ‡∏ä‡πà‡∏ô\n",
    "| row ‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠ | ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠      | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢      |\n",
    "| ----------- | ----------- |----------- |\n",
    "|  18   | `that√•√ïs`       |that's      |\n",
    "|  21 | `I¬â√õ√∑m`     |I'm |\n",
    "|    90  | `Don¬â√õ√∑t`       | Don't|\n",
    "|   90 | `you¬â√õ√∑ll`    | you'll|\n",
    "|   22  | `So √å_ pay`      | So u pay|\n",
    "| 19  | `4txt/√å¬º1.20`    | 4txt/¬£1.20|\n",
    "|  5   | `√•¬£1500`      | ¬£1500|\n",
    "| 51  |`&lt;#&gt;`    | <#> (placeholder ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏™‡πà‡πÄ‡∏•‡∏Ç)|\n",
    "| 28  |`&amp;`    | & |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a514eb-e6e0-43d2-b4dc-165f6a2026b5",
   "metadata": {},
   "source": [
    "### ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÅ‡∏õ‡∏•‡∏Å‡πÜ ‡∏û‡∏ß‡∏Å‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏´‡∏•‡∏±‡∏Å‡πÜ‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå dataset ‡∏°‡∏µ 2 ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "1. `Encoding ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (decode ‡∏ú‡∏¥‡∏î)`  \n",
    "   ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô Windows-1252 / cp1252 ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ latin1 (‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö) ‡∏ó‡∏≥‡πÉ‡∏´‡πâ byte ‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ú‡∏¥‡∏î\n",
    "   >‡πÄ‡∏•‡∏¢‡πÄ‡∏´‡πá‡∏ô √•√ï, ¬â√õ√∑, √å_, √•¬£ ‡πÅ‡∏ó‡∏ô ', ‚Äô, ¬£ ‡∏Ø‡∏•‡∏Ø\n",
    "3. `‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å escape ‡πÅ‡∏ö‡∏ö HTML`  \n",
    "   ‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏ñ‡∏π‡∏Å‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡πÉ‡∏ô‡∏£‡∏π‡∏õ entity ‡πÄ‡∏ä‡πà‡∏ô\n",
    "   > `&lt;` ‚Üí <  \n",
    "   > `&gt;` ‚Üí >  \n",
    "   > `&amp;` ‚Üí &  \n",
    "   > ‡πÄ‡∏•‡∏¢‡πÄ‡∏´‡πá‡∏ô `&lt;#&gt;` ‡πÅ‡∏•‡∏∞ `&amp;` ‡πÅ‡∏ó‡∏ô <#> ‡∏Å‡∏±‡∏ö &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5d404-5b14-434d-848d-e67e6e71f256",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "> ### üìù **Note:**  encoding='latin1' ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
    "> ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™ (encode) ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡πÑ‡∏ö‡∏ï‡πå) ‡πÅ‡∏•‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏±‡∏ö‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤ ‡∏Å‡πá‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ñ‡∏≠‡∏î‡∏£‡∏´‡∏±‡∏™ (decode) ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏î‡∏¥‡∏°\n",
    "> - `encoding:` ‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÉ‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô pd.read_csv() (‡πÅ‡∏•‡∏∞‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå) ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏≠‡∏Å‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏ñ‡∏π‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡∏∏‡∏î‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô\n",
    "> - `latin1` (‡∏´‡∏£‡∏∑‡∏≠ ISO-8859-1): ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™‡∏ä‡∏∏‡∏î‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÅ‡∏ö‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏¢‡∏∏‡πÇ‡∏£‡∏õ‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏ï‡∏Å ‡∏°‡∏±‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™‡πÅ‡∏ö‡∏ö ascii ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b9c68d-d47a-4bfb-9de9-749993133848",
   "metadata": {},
   "source": [
    "> ### üìù **Note:**  HTML escape ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
    "> ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á ‚Äú‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‚Äù ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÉ‡∏ô HTML\n",
    "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ú‡∏¥‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ó‡πá‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏Ñ‡πâ‡∏î\n",
    "> #### üîé ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á escape\n",
    ">‡πÉ‡∏ô HTML ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡πÄ‡∏ä‡πà‡∏ô\n",
    "`<` `>` `&` `\"` `'`\n",
    "‡∏ñ‡πâ‡∏≤‡πÄ‡∏≠‡∏≤‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏£‡∏á ‡πÜ ‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏à‡∏∞‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô ‡πÅ‡∏ó‡πá‡∏Å HTML ‡∏´‡∏£‡∏∑‡∏≠ ‡πÇ‡∏Ñ‡πâ‡∏î  `__‡πÄ‡∏•‡∏¢‡∏ï‡πâ‡∏≠‡∏á ‚Äúescape‚Äù ‡∏°‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô__`\n",
    ">\n",
    "| ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏à‡∏£‡∏¥‡∏á | HTML escaped | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢     |\n",
    "| ------------ | ------------ | ------------ |\n",
    "| `<`          | `&lt;`       | less than    |\n",
    "| `>`          | `&gt;`       | greater than |\n",
    "| `&`          | `&amp;`      | ampersand    |\n",
    "| `\"`          | `&quot;`     | double quote |\n",
    "| `'`          | `&#39;`      | single quote |\n",
    "| `<#>`  | `&lt;#&gt;`      | placeholder |\n",
    "\n",
    ">A gram usually runs like <#>  \n",
    ">A gram usually runs like 20  \n",
    ">üëâ `<#>` ‡πÄ‡∏õ‡πá‡∏ô placeholder ‡πÅ‡∏ó‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏£‡∏≤‡∏Ñ‡∏≤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea25107-a342-4759-ae46-709d155690a0",
   "metadata": {},
   "source": [
    "# ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏∏‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏û‡∏ö‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Dataset ‡∏ô‡∏µ‡πâ\n",
    "1. Encoding ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô (decode ‡∏ú‡∏¥‡∏î)  ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô function dict ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£\n",
    "2. ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å escape ‡πÅ‡∏ö‡∏ö HTML ‡∏ï‡πâ‡∏≠‡∏á unescape ‡∏Å‡πà‡∏≠‡∏ô\n",
    "3. ‡∏°‡∏µ‡∏•‡∏¥‡∏á‡∏Ñ‡πå‡πÄ‡∏ä‡πà‡∏ô www.dbuk.net(row 12) ‡∏´‡∏£‡∏∑‡∏≠http://wap.xxxmobilemovieclub.com?n=QJKGIGHJJGCBL( row 15) ‡πÉ‡∏´‡πâ‡∏à‡∏≥‡∏à‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á/‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô`[URL]`\n",
    "4. ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏© ‡∏û‡∏ß‡∏Å‡∏£‡∏≤‡∏Ñ‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô √•¬£100,000 (row 12) ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏ß‡∏¢ `[MONEY]`\n",
    "5. ‡∏°‡∏µ‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ‡πÄ‡∏ä‡πà‡∏ô 08452810075 ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô `[PHONE]`\n",
    "6. ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠ ‡πÄ‡∏ä‡πà‡∏ô lol, u, omg, n ‡πÉ‡∏´‡πâ‡∏Ç‡∏¢‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÄ‡∏ï‡πá‡∏°‡∏Å‡πà‡∏≠‡∏ô\n",
    "7. ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏°‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÅ‡∏•‡∏∞‡∏û‡∏¥‡∏°‡∏û‡πå‡πÉ‡∏´‡∏ç‡πà‡∏ú‡∏™‡∏°‡∏Å‡∏±‡∏ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ec2164-7c99-4682-8bec-a07f9f4639ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/natchanon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/natchanon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/natchanon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eff4085-6930-438b-9ef3-6d9a8da5a973",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227c8d1-9c84-4c30-89f8-250d178812a2",
   "metadata": {},
   "source": [
    "## ------------- 1) ‡πÅ‡∏Å‡πâ mojibake / encoding ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô ------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d069147-4548-41ef-b57c-fe0eeb5acc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACEMENTS = {\n",
    "    \"√•¬£\": \"¬£\",   # ‡πÅ‡∏Å‡πâ ¬£ ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô\n",
    "    \"√•√ï\": \"'\",   # that's, don't ‡∏Ø‡∏•‡∏Ø\n",
    "    \"√•√ì\": \"\",    \n",
    "\n",
    "    \"\\x89√õ√∑\": \"'\",   # I'm, don't, we'll ‡∏Ø‡∏•‡∏Ø\n",
    "    \"\\x89√õ_\": \" \",   # ‡∏Ç‡∏¢‡∏∞ text ‡πÅ‡∏õ‡∏•‡∏Å ‡πÜ ‡∏ï‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô space\n",
    "\n",
    "    \"√å_\": \"u\",   # So u pay, msg u, pick u ‡∏Ø‡∏•‡∏Ø\n",
    "    \"√å√è\": \"u\",   # u wait, u say, u log off\n",
    "    \"√å¬º\": \"¬£\",   # √å¬º1.20 ‚Üí ¬£1.20\n",
    "}\n",
    "\n",
    "def fix_encoding(text):\n",
    "    \"\"\"\n",
    "    ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° encoding ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô \n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):     # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà string ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° (‡∏Å‡∏±‡∏ô error)\n",
    "        return text\n",
    "\n",
    "    for bad, good in REPLACEMENTS.items():     # 2) ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å\n",
    "        text = text.replace(bad, good)\n",
    "\n",
    "    clean_text = \"\"     # 3) ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏õ‡∏•‡∏Å ‡πÜ ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠     # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ASCII ‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ ¬£\n",
    "    for ch in text:\n",
    "        if ord(ch) < 128 or ch == \"¬£\":\n",
    "            clean_text += ch\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce1d98-2d13-481c-8f11-97364a579c75",
   "metadata": {},
   "source": [
    "## ------------- 2) ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠ ------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27eb1db0-8b99-4332-a7a7-28d4221ec4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrev_dict = {\n",
    "    \"2\": \"to\",\n",
    "    \"2day\": \"today\",\n",
    "    \"2moro\": \"tomorrow\",\n",
    "    \"4\": \"for\",\n",
    "    \"abt\": \"about\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"b\": \"be\",\n",
    "    \"b4\": \"before\",\n",
    "    \"bc\": \"because\",\n",
    "    \"bcoz\": \"because\",\n",
    "    \"bf\": \"boyfriend\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"bro\": \"brother\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"c\": \"see\",\n",
    "    \"cal\": \"call\",\n",
    "    \"call\": \"call\",\n",
    "    \"cos\": \"because\",\n",
    "    \"coz\": \"because\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"cya\": \"see you\",\n",
    "    \"dun\": \"do not\",\n",
    "    \"eod\": \"end of day\",\n",
    "    \"ez\": \"easy\",\n",
    "    \"fone\": \"phone\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"gf\": \"girlfriend\",\n",
    "    \"gm\": \"good morning\",\n",
    "    \"gn\": \"good night\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"haha\": \"laugh\",\n",
    "    \"hehe\": \"laugh\",\n",
    "    \"hr\": \"hour\",\n",
    "    \"hrs\": \"hours\",\n",
    "    \"idc\": \"i do not care\",\n",
    "    \"idk\": \"i do not know\",\n",
    "    \"ikr\": \"i know right\",\n",
    "    \"ily\": \"i love you\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"k\": \"okay\",\n",
    "    \"kinda\": \"kind of\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"luv\": \"love\",\n",
    "    \"min\": \"minute\",\n",
    "    \"mins\": \"minutes\",\n",
    "    \"missu\": \"miss you\",\n",
    "    \"mob\": \"mobile\",\n",
    "    \"msg\": \"message\",\n",
    "    \"n\": \"and\",\n",
    "    \"nah\": \"no\",\n",
    "    \"nite\": \"night\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"ok\": \"okay\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"pls\": \"please\",\n",
    "    \"plz\": \"please\",\n",
    "    \"pm\": \"private message\",\n",
    "    \"rn\": \"right now\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"sec\": \"second\",\n",
    "    \"secs\": \"seconds\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"sorta\": \"sort of\",\n",
    "    \"sup\": \"what is up\",\n",
    "    \"tba\": \"to be announced\",\n",
    "    \"tbc\": \"to be confirmed\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"tdy\": \"today\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"tmr\": \"tomorrow\",\n",
    "    \"tmrw\": \"tomorrow\",\n",
    "    \"tnx\": \"thanks\",\n",
    "    \"txt\": \"text\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"u2\": \"you too\",\n",
    "    \"w\": \"with\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"wk\": \"week\",\n",
    "    \"wks\": \"weeks\",\n",
    "    \"wtf\": \"what the fuck\",\n",
    "    \"wth\": \"what the hell\",\n",
    "    \"xmas\": \"christmas\",\n",
    "    \"xoxo\": \"hugs and kisses\",\n",
    "    \"yo\": \"hey\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ee193f-6f63-435e-89ac-67ba17e6786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def expand_abbrev(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Expand abbreviations using abbrev_dict\n",
    "    - case-insensitive\n",
    "    - keep punctuation\n",
    "    - handle ¬£900, 12hrs, u., msg,\n",
    "    - avoid breaking phone numbers / ordinals\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    expanded = []\n",
    "\n",
    "    for token in tokens:\n",
    "        # ‡πÅ‡∏¢‡∏Å prefix / core / suffix\n",
    "        m = re.match(r\"^([^a-zA-Z¬£]*)([a-zA-Z¬£]+)([^a-zA-Z¬£]*)$\", token)\n",
    "        if not m:\n",
    "            expanded.append(token)\n",
    "            continue\n",
    "\n",
    "        prefix, core, suffix = m.groups()\n",
    "        key = core.lower()\n",
    "\n",
    "        # ‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡∏Ç‡∏•‡πâ‡∏ß‡∏ô / ‡πÄ‡∏•‡∏Ç‡∏ú‡∏™‡∏°‡∏Ñ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô 2nd, 12th\n",
    "        if re.search(r\"\\d\", core) and core.lower() not in abbrev_dict:\n",
    "            expanded.append(token)\n",
    "            continue\n",
    "\n",
    "        if key in abbrev_dict:\n",
    "            expanded_core = abbrev_dict[key]\n",
    "            expanded.append(prefix + expanded_core + suffix)\n",
    "        else:\n",
    "            expanded.append(token)\n",
    "\n",
    "    return \" \".join(expanded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8394fe-a0d8-47c3-8c04-4964745930d2",
   "metadata": {},
   "source": [
    "## ------------- 3) Normalization (lowercase + ‡πÅ‡∏ó‡∏ô pattern ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç) ------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "149f1e7a-2f54-4aa6-9ca4-7241fc03a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_RE   = re.compile(r'(https?://\\S+|www\\.\\S+)', flags=re.IGNORECASE)\n",
    "MONEY_RE = re.compile(r'¬£\\s?\\d+([.,]\\d+)?')   # ¬£100, ¬£1500, ¬£1.50 ‡∏Ø‡∏•‡∏Ø\n",
    "PHONE_RE = re.compile(r'\\b\\d{3,}\\b')         # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô 3 ‡∏ï‡∏±‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ (‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ö‡∏≠‡∏£‡πå/‡πÇ‡∏Ñ‡πâ‡∏î)\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    # 1) ‡πÅ‡∏Å‡πâ encoding ‡∏Å‡πà‡∏≠‡∏ô\n",
    "    text = fix_encoding(text)\n",
    "    \n",
    "    # 2) ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠\n",
    "    text = expand_abbrev(text)\n",
    "    # 3) ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å\n",
    "    text = text.lower()\n",
    "\n",
    "    # 4) ‡πÅ‡∏ó‡∏ô pattern ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏î‡πâ‡∏ß‡∏¢ token ‡∏û‡∏¥‡πÄ‡∏®‡∏©\n",
    "    text = URL_RE.sub(\" URL \", text)\n",
    "    text = MONEY_RE.sub(\" MONEY \", text)\n",
    "    text = PHONE_RE.sub(\" PHONE \", text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f01744-4dea-44f5-9dab-4b5693c514ab",
   "metadata": {},
   "source": [
    "## ------------- 4) ‡∏•‡∏ö punctuation + digit ------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "defb502a-7894-489f-acbc-d400f8e9dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc_num(text: str):\n",
    "    text = re.sub(r\"\\d+\", \" \", text) # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) # ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc0870-bf5c-4b5d-a011-6bbd2f1be786",
   "metadata": {},
   "source": [
    "## ------------- 5) Tokenize + (optional) stopwords + (optional) stemming ------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876a5d70-e0de-4439-937f-08f40ac94609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà stopword\n",
    "    return [w for w in tokens if w not in stop_words]\n",
    "\n",
    "def apply_lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c900c39b-69d2-4870-9107-1e2e47c92070",
   "metadata": {},
   "source": [
    "## ------------- 5) ‡∏£‡∏ß‡∏° pipeline ------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26044b97-6231-4535-92c2-7e2d48abec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sms(text: str,\n",
    "                   remove_sw: bool = True,\n",
    "                   do_stem: bool = True) -> str:\n",
    "    # step 1: normalize -> ‡πÅ‡∏Å‡πâ encoding + ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥ + ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å + ‡πÅ‡∏ó‡∏ô‡∏Ñ‡∏≥‡∏û‡∏¥‡πÄ‡∏®‡∏©(URL, MONEY, PHONE)\n",
    "    text = normalize_text(text)\n",
    "\n",
    "    # step 2: remove punctuation + digit\n",
    "    text = remove_punc_num(text)\n",
    "        \n",
    "    # step 3: tokenize + stopwords + stemming\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = apply_lemmatization(tokens)\n",
    "\n",
    "    return tokens\n",
    "    \n",
    "    # ‡∏£‡∏ß‡∏°‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô string \n",
    "    # return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a94c4045-178b-4153-a011-3e4f1734a93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalize_text\n",
      "xxxmobilemovieclub: to use your credit, click the wap link in the next text message or click here>>  URL  xxxmobilemovieclub.com?n=qjkgighjjgcbl.\n",
      "\n",
      "remove_punc_num\n",
      "xxxmobilemovieclub to use your credit click the wap link in the next text message or click here  URL  xxxmobilemovieclubcomnqjkgighjjgcbl\n",
      "\n",
      "tokenize\n",
      "['xxxmobilemovieclub', 'to', 'use', 'your', 'credit', 'click', 'the', 'wap', 'link', 'in', 'the', 'next', 'text', 'message', 'or', 'click', 'here', 'URL', 'xxxmobilemovieclubcomnqjkgighjjgcbl']\n",
      "\n",
      "remove_stopwords\n",
      "['xxxmobilemovieclub', 'use', 'credit', 'click', 'wap', 'link', 'next', 'text', 'message', 'click', 'URL', 'xxxmobilemovieclubcomnqjkgighjjgcbl']\n",
      "\n",
      "apply_lemmatization\n",
      "['xxxmobilemovieclub', 'use', 'credit', 'click', 'wap', 'link', 'next', 'text', 'message', 'click', 'URL', 'xxxmobilemovieclubcomnqjkgighjjgcbl']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = \"XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL.\"\n",
    "test = normalize_text(test)\n",
    "print(\"normalize_text\")\n",
    "print(test,end=\"\\n\\n\")\n",
    "\n",
    "test = remove_punc_num(test)\n",
    "print(\"remove_punc_num\")\n",
    "print(test,end=\"\\n\\n\")\n",
    "\n",
    "test = tokenize(test)\n",
    "print(\"tokenize\")\n",
    "print(test,end=\"\\n\\n\")\n",
    "\n",
    "test = remove_stopwords(test)\n",
    "print(\"remove_stopwords\")\n",
    "print(test,end=\"\\n\\n\")\n",
    "\n",
    "test = apply_lemmatization(test)\n",
    "print(\"apply_lemmatization\")\n",
    "print(test,end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "816dae16-676b-480b-8bc7-38fa52a901e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö DataFrame ‚Äì ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡∏°‡πà ‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°\n",
    "df[\"clean_text\"] = df[\"v2\"].apply(preprocess_sms)\n",
    "# # label ‡∏Å‡πá‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô 0/1 ‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏ß‡∏¢\n",
    "# df[\"label\"] = df[\"v1\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f95ce86-1e06-4fcb-ba51-ca6c6e908414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[okay, lar, joking, wif, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, wkly, comp, win, fa, cup, final,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[say, early, hor, see, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>[nd, time, tried, contact, MONEY, pound, prize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will √å_ b going to esplanade fr home?</td>\n",
       "      <td>[going, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>[pity, mood, soany, suggestion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>[guy, bitching, acted, like, id, interested, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>[rolling, floor, laughing, true, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...   \n",
       "1      ham                      Ok lar... Joking wif u oni...   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      ham  U dun say so early hor... U c already then say...   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "...    ...                                                ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5568   ham              Will √å_ b going to esplanade fr home?   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...   \n",
       "5571   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                             clean_text  \n",
       "0     [go, jurong, point, crazy, available, bugis, g...  \n",
       "1                         [okay, lar, joking, wif, oni]  \n",
       "2     [free, entry, wkly, comp, win, fa, cup, final,...  \n",
       "3                  [say, early, hor, see, already, say]  \n",
       "4          [dont, think, go, usf, life, around, though]  \n",
       "...                                                 ...  \n",
       "5567  [nd, time, tried, contact, MONEY, pound, prize...  \n",
       "5568                       [going, esplanade, fr, home]  \n",
       "5569                    [pity, mood, soany, suggestion]  \n",
       "5570  [guy, bitching, acted, like, id, interested, b...  \n",
       "5571             [rolling, floor, laughing, true, name]  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923bed1a-8c3c-4a88-b10e-4fed9d3749b6",
   "metadata": {},
   "source": [
    "# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥ Text Preprocessing ‡∏Å‡∏±‡∏ö dataset ‡∏ô‡∏µ‡πâ ‡∏°‡∏µ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\n",
    "1) ‡∏ï‡∏±‡∏î cloumn ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏¥‡πâ‡∏á\n",
    "2) fix encoding ‡πÅ‡∏•‡∏∞  fix escape html\n",
    "3) ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠ ‡πÄ‡∏ä‡πà‡∏ô  btw => by the way\n",
    "4) ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å\n",
    "5) ‡πÅ‡∏ó‡∏ô URL ‡∏î‡πâ‡∏ß‡∏¢ <'URL'>\n",
    "6) ‡πÅ‡∏ó‡∏ô ‡πÄ‡∏ö‡∏≠‡∏£‡πå‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå ‡∏î‡πâ‡∏ß‡∏¢ <'PHONE'>\n",
    "7) ‡πÅ‡∏ó‡∏ô ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏á‡∏¥‡∏ô‡∏î‡πâ‡∏ß‡∏¢ <'MONEY'>\n",
    "8) ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
    "9) ‡∏•‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ punctuation\n",
    "10) ‡∏ï‡∏±‡∏î space ‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥\n",
    "11) ‡πÅ‡∏ö‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥ ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ tokenization\n",
    "12) remove stopwords\n",
    "13) lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e616a0e-3598-4d30-8ac5-7b36b23e4dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
