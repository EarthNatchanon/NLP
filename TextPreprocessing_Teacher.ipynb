{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fncJZps8v6Iu"
   },
   "source": [
    "# การประมวลผลข้อความเบื้องต้น\n",
    "\n",
    "## Basic String Method\n",
    "* text.lower() : convert to lowercase\n",
    "* text.split() : Splits a string into a list of words\n",
    "* text.strip() : Removes leading/trailing whitespace\n",
    "* text.replace(old, new) : Replaces occurrences of old with new\n",
    "* text.startswith(prefix) / text.endswith(suffix)\n",
    "* text.find(substring): return the index of substring or -1\n",
    "* text.count(substring)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/natchanon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/natchanon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/natchanon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7oskjIAF2YGG"
   },
   "outputs": [],
   "source": [
    "text = '''Check out this <b>amazing</b> website: https://www.example.com/page1?id=123 for more info! It has 100+ articles. The <p>first</p> one is great. Also, don't forget to check out the other pages. ummm, like, uh, really good stuff. And this is just the beginning!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZTCQJB6W2amr",
    "outputId": "6c6e07ef-690a-4d76-927e-8460645ec53c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Check',\n",
       " 'out',\n",
       " 'this',\n",
       " '<b>amazing</b>',\n",
       " 'website:',\n",
       " 'https://www.example.com/page1?id=123',\n",
       " 'for',\n",
       " 'more',\n",
       " 'info!',\n",
       " 'It',\n",
       " 'has',\n",
       " '100+',\n",
       " 'articles.',\n",
       " 'The',\n",
       " '<p>first</p>',\n",
       " 'one',\n",
       " 'is',\n",
       " 'great.',\n",
       " 'Also,',\n",
       " \"don't\",\n",
       " 'forget',\n",
       " 'to',\n",
       " 'check',\n",
       " 'out',\n",
       " 'the',\n",
       " 'other',\n",
       " 'pages.',\n",
       " 'ummm,',\n",
       " 'like,',\n",
       " 'uh,',\n",
       " 'really',\n",
       " 'good',\n",
       " 'stuff.',\n",
       " 'And',\n",
       " 'this',\n",
       " 'is',\n",
       " 'just',\n",
       " 'the',\n",
       " 'beginning!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_9RbqFa3HM2",
    "outputId": "6061b983-49f4-427d-ab68-abd7f8fd162a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Check out this <b>amazing</b> website: https://www.example.com/page1?id=123 for more info!',\n",
       " 'It has 100+ articles.',\n",
       " 'The <p>first</p> one is great.',\n",
       " \"Also, don't forget to check out the other pages.\",\n",
       " 'ummm, like, uh, really good stuff.',\n",
       " 'And this is just the beginning!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e2aRDN634w7",
    "outputId": "cb06a13b-de30-4f3d-f0b6-000e7f866e08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Check',\n",
       " 'out',\n",
       " 'this',\n",
       " '<',\n",
       " 'b',\n",
       " '>',\n",
       " 'amazing',\n",
       " '<',\n",
       " '/b',\n",
       " '>',\n",
       " 'website',\n",
       " ':',\n",
       " 'https',\n",
       " ':',\n",
       " '//www.example.com/page1',\n",
       " '?',\n",
       " 'id=123',\n",
       " 'for',\n",
       " 'more',\n",
       " 'info',\n",
       " '!',\n",
       " 'It',\n",
       " 'has',\n",
       " '100+',\n",
       " 'articles',\n",
       " '.',\n",
       " 'The',\n",
       " '<',\n",
       " 'p',\n",
       " '>',\n",
       " 'first',\n",
       " '<',\n",
       " '/p',\n",
       " '>',\n",
       " 'one',\n",
       " 'is',\n",
       " 'great',\n",
       " '.',\n",
       " 'Also',\n",
       " ',',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'forget',\n",
       " 'to',\n",
       " 'check',\n",
       " 'out',\n",
       " 'the',\n",
       " 'other',\n",
       " 'pages',\n",
       " '.',\n",
       " 'ummm',\n",
       " ',',\n",
       " 'like',\n",
       " ',',\n",
       " 'uh',\n",
       " ',',\n",
       " 'really',\n",
       " 'good',\n",
       " 'stuff',\n",
       " '.',\n",
       " 'And',\n",
       " 'this',\n",
       " 'is',\n",
       " 'just',\n",
       " 'the',\n",
       " 'beginning',\n",
       " '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2B7j0adU6By8",
    "outputId": "9db86130-91ac-484d-c9f9-d008cd646a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a, about, above, after, again, against, ain, all, am, an, and, any, are, aren, aren't, as, at, be, because, been, before, being, below, between, both, but, by, can, couldn, couldn't, d, did, didn, didn't, do, does, doesn, doesn't, doing, don, don't, down, during, each, few, for, from, further, had, hadn, hadn't, has, hasn, hasn't, have, haven, haven't, having, he, he'd, he'll, her, here, hers, herself, he's, him, himself, his, how, i, i'd, if, i'll, i'm, in, into, is, isn, isn't, it, it'd, it'll, it's, its, itself, i've, just, ll, m, ma, me, mightn, mightn't, more, most, mustn, mustn't, my, myself, needn, needn't, no, nor, not, now, o, of, off, on, once, only, or, other, our, ours, ourselves, out, over, own, re, s, same, shan, shan't, she, she'd, she'll, she's, should, shouldn, shouldn't, should've, so, some, such, t, than, that, that'll, the, their, theirs, them, themselves, then, there, these, they, they'd, they'll, they're, they've, this, those, through, to, too, under, until, up, ve, very, was, wasn, wasn't, we, we'd, we'll, we're, were, weren, weren't, we've, what, when, where, which, while, who, whom, why, will, with, won, won't, wouldn, wouldn't, y, you, you'd, you'll, your, you're, yours, yourself, yourselves, you've\n"
     ]
    }
   ],
   "source": [
    "print(\", \".join(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSBB78z97sAj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yt1r-mub7HV8",
    "outputId": "82a36430-22b5-4d0b-95a1-f7dc6f8f626e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEN9o6UI787X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7klOiGj0tBO"
   },
   "source": [
    "#### **ตัวอย่างสร้าง Function ทำ text preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ngzaAORQyFh4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is a Sample Text! With, some punctuation.\n",
      "Processed Text: this is a sample text with some punctuation\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Removing extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Example\n",
    "text = \"This is a Sample Text! With, some punctuation.\"\n",
    "processed_text = preprocess_text(text)\n",
    "print(f\"Original Text: {text}\")\n",
    "print(f\"Processed Text: {processed_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SKt5OoPJ2hef"
   },
   "outputs": [],
   "source": [
    "englist_text = \"Check out this <b>amazing</b> website: https://www.example.com/page1?id=123 for more info! It has 100+ articles. The <p>first</p> one is great. Also, don't forget to check out the other pages. ummm, like, uh, really good stuff. And this is just the beginning!\"\n",
    "thai_text =  \"ไปเที่ยวทะเลกันไหมครับ? ดูรีวิวที่นี่เลยครับ https://www.example.com/travel/beach <b>ราคาเพียง</b> 2,500 บาท! มีโปรโมชั่นลดอีก 10% ด้วยนะ หรือจะไปที่ <a href='link2'>เกาะสมุย</a> ก็ได้นะ สนใจติดต่อได้ที่เบอร์ 081-123-4567 อย่าลืมพกครีมกันแดดด้วยนะจ๊ะ #เที่ยวทะเล #ทริปทะเล เดี๋ยวนี้ไปง่ายๆ สะดวกสุดๆ\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00TH4i8p1CuH"
   },
   "source": [
    "### ทดลองทำ\n",
    "\n",
    "* Tokenization\n",
    "* Stop Words Removal\n",
    "* Stemming และ Lemmatization\n",
    "* Lowercase\n",
    "* Punctuation Removal\n",
    "* Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rTrilq7nOPel"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrJS_wP9T1z2",
    "outputId": "ee402388-f11d-4161-d2a0-30d9f1d865a2"
   },
   "outputs": [],
   "source": [
    "!pip install -q pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7rWcxFVZ9Fx",
    "outputId": "9d32fe4d-02b7-41ee-b773-61fd2e43194e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepcut in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (0.7.0.0)\n",
      "Requirement already satisfied: tensorflow>=2.0.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from deepcut) (2.20.0)\n",
      "Requirement already satisfied: pandas in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from deepcut) (2.3.3)\n",
      "Requirement already satisfied: scipy in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from deepcut) (1.16.3)\n",
      "Requirement already satisfied: numpy in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from deepcut) (2.3.5)\n",
      "Requirement already satisfied: scikit-learn in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from deepcut) (1.8.0)\n",
      "Requirement already satisfied: h5py in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from deepcut) (3.15.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (6.33.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (3.12.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorflow>=2.0.0->deepcut) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.0.0->deepcut) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.0.0->deepcut) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.0.0->deepcut) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.0.0->deepcut) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow>=2.0.0->deepcut) (3.10)\n",
      "Requirement already satisfied: pillow in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow>=2.0.0->deepcut) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow>=2.0.0->deepcut) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow>=2.0.0->deepcut) (3.1.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow>=2.0.0->deepcut) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from keras>=3.10.0->tensorflow>=2.0.0->deepcut) (14.2.0)\n",
      "Requirement already satisfied: namex in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from keras>=3.10.0->tensorflow>=2.0.0->deepcut) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from keras>=3.10.0->tensorflow>=2.0.0->deepcut) (0.18.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow>=2.0.0->deepcut) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from pandas->deepcut) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from pandas->deepcut) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from pandas->deepcut) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow>=2.0.0->deepcut) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow>=2.0.0->deepcut) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow>=2.0.0->deepcut) (0.1.2)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from scikit-learn->deepcut) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages (from scikit-learn->deepcut) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install deepcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uwfhVwqi1SFq"
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "import pythainlp\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6gSKIEZu1VJl",
    "outputId": "b4394570-ca33-4c2b-e390-538b4e938e62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/natchanon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/natchanon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/natchanon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/natchanon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/natchanon/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download useful corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbRMKSCJN2Hr",
    "outputId": "c6d0b171-6af6-478f-a669-cf0644a5d2c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['albanian', 'arabic', 'azerbaijani', 'basque', 'belarusian', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'tamil', 'turkish']\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5VjJWc1pJox",
    "outputId": "6465f0dc-e70a-4d31-e48b-1a2db5d34fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization (Thai): ['ฉัน', 'รัก', 'ภาษาไทย']\n",
      "Tokenization (English): ['I', 'loved', 'programming', 'in', 'Python', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "text_th = \"ฉันรักภาษาไทย\"\n",
    "tokens_th = pythainlp.tokenize.word_tokenize(text_th)\n",
    "print(\"Tokenization (Thai):\", tokens_th)\n",
    "\n",
    "text_en = \"I loved programming in Python.\"\n",
    "tokens_en = word_tokenize(text_en)\n",
    "print(\"Tokenization (English):\", tokens_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFjW8cDI7jXN",
    "outputId": "12ba0f64-d23f-4908-da9d-0a7da71f3f76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 14:11:46.247038: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization (English): ['Check', 'out', 'this', '<', 'b', '>', 'amazing', '<', '/b', '>', 'website', ':', 'https', ':', '//www.example.com/page1', '?', 'id=123', 'for', 'more', 'info', '!', 'It', 'has', '100+', 'articles', '.', 'The', '<', 'p', '>', 'first', '<', '/p', '>', 'one', 'is', 'great', '.', 'Also', ',', 'do', \"n't\", 'forget', 'to', 'check', 'out', 'the', 'other', 'pages', '.', 'ummm', ',', 'like', ',', 'uh', ',', 'really', 'good', 'stuff', '.', 'And', 'this', 'is', 'just', 'the', 'beginning', '!']\n",
      "Tokenization (Thai) default: ['ฉัน', 'รัก', 'ภาษาไทย']\n",
      "Tokenization (Thai) longest: ['ไปเที่ยว', 'ทะเล', 'กัน', 'ไหม', 'ครับ', '?', ' ', 'ดู', 'รีวิว', 'ที่นี่', 'เลย', 'ครับ', ' ', 'https', '://', 'www', '.', 'example', '.', 'com', '/', 'travel', '/', 'beach', ' <', 'b', '>', 'ราคา', 'เพียง', '</', 'b', '>', ' ', '2,500', ' ', 'บาท', '!', ' ', 'มี', 'โปรโมชั่น', 'ลด', 'อีก', ' ', '10', '%', ' ', 'ด้วย', 'นะ', ' ', 'หรือ', 'จะ', 'ไป', 'ที่', ' <', 'a', ' ', 'href', \"='\", 'link2', \"'>\", 'เกาะ', 'สมุย', '</', 'a', '>', ' ', 'ก็ได้', 'นะ', ' ', 'สนใจ', 'ติดต่อ', 'ได้ที่', 'เบอร์', ' ', '081', '-', '123', '-', '4567', ' ', 'อย่า', 'ลืม', 'พก', 'ครีมกันแดด', 'ด้วย', 'นะ', 'จ๊ะ', ' #', 'เที่ยว', 'ทะเล', ' #', 'ทริป', 'ทะเล', ' ', 'เดี๋ยวนี้', 'ไป', 'ง่ายๆ', ' ', 'สะดวก', 'สุดๆ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 14:11:46.306298: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-12 14:11:47.968018: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765523508.888848    4599 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/natchanon/anaconda3/envs/nlp_env/lib/python3.11/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-12-12 14:11:50.748609: I external/local_xla/xla/service/service.cc:163] XLA service 0x7dd8f40022d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-12 14:11:50.748649: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-12-12 14:11:50.887228: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-12 14:11:51.066262: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 5s/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765523514.687543    4719 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 529ms/step\n",
      "Tokenization (Thai) newmm: ['ไป', 'เที่ยว', 'ทะเล', 'กัน', 'ไหม', 'ครับ', '?', ' ', 'ดู', 'รี', 'วิว', 'ที่', 'นี่', 'เลย', 'ครับ', ' ', 'https://www.example.com/travel/beach', ' <b>ราคาเพียง', '</b>', ' ', '2,500', ' ', 'บาท', '!', ' ', 'มี', 'โปรโมชั่น', 'ลด', 'อีก', ' ', '10', '%', ' ', 'ด้วย', 'นะ', ' ', 'หรือ', 'จะ', 'ไป', 'ที่', \" <a href='link\", \"2'>เกาะสมุย</a>\", ' ', 'ก็ได้', 'นะ', ' ', 'สนใจ', 'ติดต่อ', 'ได้', 'ที่', 'เบอร์', ' ', '081', '-', '123', '-', '4567', ' ', 'อย่า', 'ลืม', 'พก', 'ครีม', 'กัน', 'แดด', 'ด้วย', 'นะ', 'จ๊ะ', ' ', '#เที่ยว', 'ทะเล', ' ', '#ทริปทะเล', ' ', 'เดี๋ยวนี้', 'ไป', 'ง่าย', 'ๆ', ' ', 'สะดวก', 'สุด', 'ๆ']\n"
     ]
    }
   ],
   "source": [
    "# ทดสอบกับ englist_text และ thai_text\n",
    "tokens_en = word_tokenize(englist_text)\n",
    "print(\"Tokenization (English):\", tokens_en)\n",
    "\n",
    "tokens_th1 = pythainlp.tokenize.word_tokenize(thai_text)\n",
    "print(\"Tokenization (Thai) default:\", tokens_th)\n",
    "\n",
    "tokens_th2 = pythainlp.tokenize.word_tokenize(thai_text, engine=\"longest\")\n",
    "print(\"Tokenization (Thai) longest:\", tokens_th2)\n",
    "\n",
    "tokens_th3 = pythainlp.tokenize.word_tokenize(thai_text, engine=\"deepcut\")\n",
    "print(\"Tokenization (Thai) newmm:\", tokens_th3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07BI9jt21rYI",
    "outputId": "5bef944c-a3ce-4730-fe30-7f4db24ebae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words Removal (English): ['Check', '<', 'b', '>', 'amazing', '<', '/b', '>', 'website', ':', 'https', ':', '//www.example.com/page1', '?', 'id=123', 'info', '!', 'It', '100+', 'articles', '.', 'The', '<', 'p', '>', 'first', '<', '/p', '>', 'one', 'great', '.', 'Also', ',', \"n't\", 'forget', 'check', 'pages', '.', 'ummm', ',', 'like', ',', 'uh', ',', 'really', 'good', 'stuff', '.', 'And', 'beginning', '!']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "#stopwords_th = list()\n",
    "#filtered_tokens_th = [........................]\n",
    "#print(\"Stop Words Removal (Thai):\", filtered_tokens_th)\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "filtered_tokens_en = [token for token in tokens_en if token not in stopwords_en]\n",
    "print(\"Stop Words Removal (English):\", filtered_tokens_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "r7s6OD5h-Ovo"
   },
   "outputs": [],
   "source": [
    "#list comprehension\n",
    "filtered_tokens_en = [token for token in tokens_en if token not in stopwords_en]\n",
    "\n",
    "filtered_tokens_en = []\n",
    "for token in tokens_en:\n",
    "    if token not in stopwords_en:\n",
    "        filtered_tokens_en.append(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFRKSQYniObs"
   },
   "source": [
    "#### Example of stemming\n",
    "\n",
    "connect -> connect\n",
    "connects ->\n",
    "connected ->\n",
    "connection ->\n",
    "connecting ->\n",
    "\n",
    "study  ->\n",
    "studies ->\n",
    "studied ->\n",
    "studying ->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Check < b > amazing < /b > website : https : //www.example.com/page1 ? id=123 info ! It 100+ articles . The < p > first < /p > one great . Also , n't forget check pages . ummm , like , uh , really good stuff . And beginning !\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens_en\n",
    "filtered_tokens_en = \" \".join(filtered_tokens_en)\n",
    "filtered_tokens_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc_num(text: str):\n",
    "    text = re.sub(r\"\\d+\", \" \", text) # ลบตัวเลข\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) # ลบเครื่องหมาย\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check  b  amazing  b  website  https  wwwexamplecompage   id  info  It   articles  The  p  first  p  one great  Also  nt forget check pages  ummm  like  uh  really good stuff  And beginning '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens_en = remove_punc_num(filtered_tokens_en)\n",
    "filtered_tokens_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check b amazing b website https wwwexamplecompage id info It articles The p first p one great Also nt forget check pages ummm like uh really good stuff And beginning'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens_en = re.sub(r\"\\s+\", \" \", filtered_tokens_en).strip()\n",
    "filtered_tokens_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens_en = tokenize(filtered_tokens_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Check', 'b', 'amazing', 'b', 'website', 'https', 'wwwexamplecompage', 'id', 'info', 'It', 'articles', 'The', 'p', 'first', 'p', 'one', 'great', 'Also', 'nt', 'forget', 'check', 'pages', 'ummm', 'like', 'uh', 'really', 'good', 'stuff', 'And', 'beginning']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens_en\n",
    "print(filtered_tokens_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-y9xNEb1uHm",
    "outputId": "aa94c814-67f7-432a-c608-1669aa008ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming (English): ['check', 'b', 'amaz', 'b', 'websit', 'http', 'wwwexamplecompag', 'id', 'info', 'it', 'articl', 'the', 'p', 'first', 'p', 'one', 'great', 'also', 'nt', 'forget', 'check', 'page', 'ummm', 'like', 'uh', 'realli', 'good', 'stuff', 'and', 'begin']\n"
     ]
    }
   ],
   "source": [
    "# Stemming (English)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens_en = [stemmer.stem(token) for token in filtered_tokens_en]\n",
    "print(\"Stemming (English):\", stemmed_tokens_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G2KbTEye1wRO",
    "outputId": "d802e83e-11c6-4852-e011-422ca5bda910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization (English): ['Check', 'b', 'amazing', 'b', 'website', 'http', 'wwwexamplecompage', 'id', 'info', 'It', 'article', 'The', 'p', 'first', 'p', 'one', 'great', 'Also', 'nt', 'forget', 'check', 'page', 'ummm', 'like', 'uh', 'really', 'good', 'stuff', 'And', 'beginning']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization (English)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens_en = [lemmatizer.lemmatize(token) for token in filtered_tokens_en]\n",
    "print(\"Lemmatization (English):\", lemmatized_tokens_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TiL8XJtkAHU"
   },
   "source": [
    "#### Example of Lemmatization\n",
    "\n",
    "running -> run,\n",
    "studies -> study,\n",
    "happier -> happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKNRZsYjj_2k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "j2t4GPjK1y_4"
   },
   "outputs": [],
   "source": [
    "# อธิบายความแตกต่างระหว่าง Stemming vs Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swK_c4uxE6GW",
    "outputId": "1e3bea8f-452c-45a5-a7c0-c6e70f9744c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found phone number: 555-123-4567\n",
      "All matches: ['555-123-4567', '123-456-7890']\n",
      "Call me at [TELNo] or [TELNo]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Compile the pattern into a re.Pattern object\n",
    "phone_pattern = re.compile(r\"\\d{3}-\\d{3}-\\d{4}\")\n",
    "\n",
    "text = \"Call me at 555-123-4567 or 123-456-7890\"\n",
    "\n",
    "# Use methods on the compiled pattern object\n",
    "match = phone_pattern.search(text)\n",
    "if match:\n",
    "    print(f\"Found phone number: {match.group()}\") # Output: Found phone number: 555-123-4567\n",
    "\n",
    "all_matches = phone_pattern.findall(text)\n",
    "print(f\"All matches: {all_matches}\") # Output: All matches: ['555-123-4567', '123-456-7890']\n",
    "\n",
    "print(phone_pattern.sub(\"[TELNo]\", text)) # Output: Call me at PHONE or PHONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "WsaH8JXr2MYy",
    "outputId": "ed1da59f-0534-439b-c7ea-cbc63295c5c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('https?://\\\\S+|www\\\\.\\\\S+')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Check out this <b>amazing</b> website: [URL] for more info! It has 100+ articles. The <p>first</p> one is great. Also, don't forget to check out the other pages. ummm, like, uh, really good stuff. And this is just the beginning!\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL Removal\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    print(url_pattern)\n",
    "    return url_pattern.sub('[URL]', text)\n",
    "\n",
    "remove_urls(englist_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GymBrbG2GPNw",
    "outputId": "34e5ab17-da68-4ccc-ecd7-e9009247a280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuiEFl5WgtyP",
    "outputId": "bdc1f65b-2d53-43f2-f5a5-50dea0bfbbdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is a Sample Text! With, some punctuation.\n",
      "Processed Text: This is a Sample Text With some punctuation\n"
     ]
    }
   ],
   "source": [
    "# prompt: # remove punctuation using string punctuation\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punctuation_string(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a Sample Text! With, some punctuation.\"\n",
    "processed_text = remove_punctuation_string(text)\n",
    "print(f\"Original Text: {text}\")\n",
    "print(f\"Processed Text: {processed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "rRpNwe-x26uA",
    "outputId": "3edd9fd3-6842-4b2a-c037-e210f9969d76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check out this bamazingb website httpswwwexamplecompage1id123 for more info It has 100 articles The pfirstp one is great Also dont forget to check out the other pages ummm like uh really good stuff And this is just the beginning'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Punctuation Removal\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "remove_punctuation(englist_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "N14ZAwWF2-KH"
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "# 2morrow -> tomorrow\n",
    "# b4  -> before\n",
    "# btw -> by the way\n",
    "# omg -> oh my god\n",
    "# :-D-> (smile)\n",
    "# 8/12/68 -> 08/12/2025\n",
    "# 1st -> first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59gLyPOD29l2"
   },
   "source": [
    "### สร้าง Function สำหรับ text preprocessing ภาษาอังกฤษ และภาษาไทย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "m4GOZm3Q6o1r"
   },
   "outputs": [],
   "source": [
    "# function text preprocessing for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9SZ-bQmlnPK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "tcelgltL7C67"
   },
   "outputs": [],
   "source": [
    "# function text preprocessing for Thai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve-8BDib7Hs5"
   },
   "source": [
    "### ประยุกต์ใช้กับ Dataset Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5GdDEKOT7GZO"
   },
   "outputs": [],
   "source": [
    "# download dataset from kaggle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5An3WZsKneQ",
    "outputId": "986399d0-3825-4b8e-aaf2-5f750433011d"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/gdrive\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2650
    },
    "id": "KAC-nEdlJGgj",
    "outputId": "ab774fa9-9783-42de-83c8-404d52b55ae5"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/gdrive/MyDrive/517432/datasets/SMSSpamCollection.txt', sep='\\t', names=['label', 'message'])\n",
    "display(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv4KFgjEMyB2"
   },
   "source": [
    "ข้อสังเกตที่ค้นพบเกี่ยวกับ Dataset นี้\n",
    "- มีสัญลักษณ์ทีใช้ใน html ปนอยู่ เช่น &lt;#&gt; (row 44) (ลบทิ้ง)\n",
    "- มีการใช้คำย่อ เช่น Lol  (ทำ normalization ไหม?)\n",
    "- มีแต่ตัวอักษรภาษาอังกฤษ ไม่มีภาษาอื่นปน (ไม่มั่นใจ ต้องเขียนโค้ดทดสอบ)\n",
    "- มีทั้งตัวอักษรตัวเล็ก ตัวใหญ่ (เปลี่ยนเป็น lowercase)\n",
    "- มี URL ปรากฎอยู่ เช่น http://wap. xxxmobilemovieclub.com (row 15), www.dbuk.net (row 12) (กำจัดทิ้ง/เปลี่ยนเป็น [URL])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5CkqvlnI5z3"
   },
   "source": [
    "ขั้นตอนที่ทำ Text Preprocessing กับ dataset นี้ มีลำดับดังนี้\n",
    "\n",
    "\n",
    "1.   List item\n",
    "2.   List item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQnhT3g87U7Q"
   },
   "outputs": [],
   "source": [
    "# apply function with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
